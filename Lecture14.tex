\documentclass[DIV=14,titlepage=false]{scrreprt}
\usepackage{parskip}
\input{preamble.tex}


\title{%
R300 Econometrics}
\author{Metrics Enjoyers}
\date{Michaelmas Term, 2023-2024}

\setuptoc{toc}{leveldown}
\setcounter{chapter}{13}

\begin{document}

\chapter{2SLS. Control Function. Endogeneity and overidentification tests.}

\section{Under, just and overidentification}

Consider again the linear regression model, with \(\vec x_{1i} \text{ exogenous and } \vec x_{2i}\) endogenous.
\[y_i = \beta_0 + x_{1i}'\beta_1 + x_{2i}'\beta_2 + u_i\]
Then take instrument:
\[w_i=\begin{pmatrix}x_{1i} \\ z_i\end{pmatrix}\]
with \(x_{1i}\) instrumenting for themselves (included exogenous variables) and \(z_i\) instrumenting for \(x_{2i}\) (excluded exogenous variables). 

If \(w_i\) \(l\)-dimensional and \(x_i\) \(k\)-dimensional:
\[\underbrace{E[w_iy_i]}_{l\times 1} = \underbrace{E[w_ix_i']}_{l\times k}\underbrace{\beta}_{k\times 1}\]
\begin{itemize}
\item If \(l<k\), then we have \textbf{underidentification}
\item If \(l=k\), then we have \textbf{just identification}
\item If \(l>k\), then we have \textbf{overidentification}
\end{itemize} 

The relevance condition, \(E[w_ix_i']\) full column rank, \underline{rules out} underidentification. This is because now l rows will be less than k columns, and since column rank = row rank, we must have deficient column rank.
\\ If \(l<k\) we have more equations than unknowns and \(E[w_ix_i']\) is no longere invertible. We could throw away extra variables but better instead to use 2SLS, since we want to extract as much exogenous variation from our endogenous variables as possible.

\section{2SLS}

For now assume \(E[\epsilon_i|w_i]=0\). Then:
\[0=E[\epsilon_i|w_i]=E[y_i-x_i'\beta|w_i]=E[y_i|w_i]-E[x_i'|w_i]\beta\]
\[\implies E[y_i|w_i]=E[x_i'|w_i]\beta\]
Suppose we also know
\[E[x_i'|w_i]=w_i'\pi\]
Then we have:
\[E[y_i|w_i]=(w_i'\pi)\beta\]

This suggest the following procedure:

\begin{definition} \textbf{\underline{2SLS}}
    \\
    \\ \underline{\smash{Stage 1:}}
    \\ Regress \(X_{n\times k} \text{ on } W_{n\times l}\) to get \(\hat\pi=(W'W)^{-1}W'X\)
    \\ Use the results to form \(\hat X = W\hat\pi\)
    \\ Note: \(\hat X=W\hat\pi=W(W'W)^{-1}W'X=P_W X\)
    \\ For the exogenous variables columns in \(\hat X\) this will correspond exactly to the original values, but for the endogenous variables columns, they will be formed as a linear combination of both the relevant instruments and exogenous variables.
    \\
    \\ \underline{\smash{Stage 2:}}
    \\ Regress \(Y_{n\times 1} \text{ on } \hat X_{n\times k}\) to find:
     \[\hat\beta_{2SLS}=(\hat X'\hat X)^{-1}\hat X'Y=(X'P_W'P_WX)^{-1}X'P_W'Y\]
     \[=(X'P_WX)^{-1}X'P_WY\]
\end{definition}
\vspace{5mm}
Consider the following IV assumptions for the model \(y_i=x_i'\beta+\epsilon_i\):
\begin{itemize}
    \item (IV0) \(y_i, x_i, w_i\) is an i.i.d sequence
    \item (IV1) \(E[w_iw_i']<\infty\) non-singular; \(E[w_ix_i']\) has full column rank (relevance)
    \item (IV2) \(E[\epsilon_i|w_i]=0\) \((\implies)\) (IV2') \(E(w_i\epsilon_i)=0\) (exogeneity)
    \item (IV3) \(E[\epsilon_i^2|w_i]=\sigma^2\) (homoskedasticity) or (IV3') \(V=Var(w_i\epsilon_i)\) is finite non singular
    \\ (Under IV(3): \(V=E[w_iw_i'\epsilon_i^2]-0=E[E[w_iw_i'\epsilon_i^2|w_i]]=\sigma^2E[w_iw_i'])\)
\end{itemize}
\vspace{5mm}
\begin{theorem} \textbf{2SLS consistency}
    \\ \
    \\ Under IV(0) IV(1) IV(2')
    \[\hat\beta_{2SLS}\xrightarrow{p}\beta\]
\end{theorem}
\vspace{5mm}
\begin{proof}
\[\hat{\beta}_{2SLS}=(\hat X'\hat X)^{-1}(\hat X Y)=(X'P_WX)^{-1}X'P_WY\]
\[=\beta+(X'P_WX)^{-1}X'P_W\epsilon\]
\[\hat{\beta}_{2SLS}-\beta=[X'W (W'W)^{-1}W'X]^{-1}X'W(W'W)^{-1}W'\epsilon\]
\[=\left[\frac{1}{n}\sum x_iw_i'(\frac{1}{n}\sum w_iw_i')^{-1}\frac{1}{n}\sum w_ix_i'\right]^{-1}\frac{1}{n}\sum x_iw_i'(\frac{1}{n}\sum w_iw_i')^{-1}(\frac{1}{n}\sum w_i\epsilon_i)\]
\[\xrightarrow{p} [E(x_iw_i')E(w_iw_i')^{-1}E(w_ix_i')]^{-1}E(x_iw_i')E(w_iw_i')^{-1}E(w_i\epsilon_i)\]
By IV(2'), \(E(w_i\epsilon_i)=0\) and by IV(1) \(E(w_iw_i')\) is non-singular to a finite constant matrix (also assume \(E(x_iw_i') <\infty\)).
Thus \[\hat\beta_{2SLS}-\beta\xrightarrow{p}0\]
\end{proof}

In general \(dim W\neq dim X\). In the case where they do: \(\hat\beta_{2SLS}\equiv\hat\beta_{IV}\), since \(W'X\) now invertible.
The 2SLS procedure ensures that \(dim \hat X=dimX\), so that \(\hat\beta_{2SLS}\equiv\hat\beta_{IV}\), using \(\hat X\) as an instrument.
Explicitly: \((X'P_WX)^{-1}X'P_WY=(X'P_W'X)^{-1}X'P_W'Y=(\hat X'X)^{-1}(\hat X'Y)=\hat\beta_{IV}\)

\begin{theorem}
    Let \(C=E[w_iw_i']\) and \(D=E[w_ix_i']\). Under IV0-1-2'-3':
    \[\sqrt{n}(\hat\beta_{2SLS}-\beta)\xrightarrow{d}N(0,(D'C^{-1}D)^{-1}D'C^{-1}VC^{-1}D(D'C^{-1}D)^{-1})\]
    where \(V=Var(w_i\epsilon_i)\)
\end{theorem}
\vspace{5mm}
\begin{proof}
\[\sqrt{n}(\hat\beta_{2SLS}-\beta)=\]
\[\left[\frac{1}{n}\sum x_iw_i'(\frac{1}{n}\sum w_iw_i')^{-1}\frac{1}{n}\sum w_ix_i'\right]^{-1}\frac{1}{n}\sum x_iw_i'(\frac{1}{n}\sum w_iw_i')^{-1}(\frac{1}{\sqrt{n}}\sum w_i\epsilon_i)\]
By Lindeberg-Levy CLT:
\[\frac{1}{\sqrt{n}}\sum w_i\epsilon_i\xrightarrow{d}N(0,V)\]
By Slutsky's theorem:
\[\xrightarrow{d} [D'C^{-1}D]^{-1}D'C^{-1}N(0,V)\]
\[= N(0,(D'C^{-1}D)^{-1}D'C^{-1}VC^{-1}D(D'C^{-1}D)^{-1})\]
Under (IV3) (homoskedasticity):
\[V=Var(w_i\epsilon_i)=E[w_iw_i'\epsilon_i^2]-0=\sigma^2E[w_iw_i']=\sigma^2C\]
Thus much of the asymptotic variance cancels, leaving
\[\sqrt{n}(\hat\beta_{2SLS}-\beta)\xrightarrow{d}N(0,\sigma^2(D'C^{-1}D)^{-1})\]
\end{proof}

\begin{note}
    In general for two full column rank conformable matrices \(A, B\):
    \\ We have \(AB\) full column rank. \\
    \\ \underline{Proof:} 
    Suppose \(AB\) not full column rank. 
    \\ Then \(\exists\) \(x\neq0\) such that \(ABx=0\) (by the rank-nullity theorem). 
    \\ \(\implies Bx\neq0\) as \(B\) full rank implies its null space is only \(\{0\}\). 
    \\ \(\implies A(Bx)\neq0\) as \(A\) also full rank with only trivial null space.
    \\ Contradiction \\ \\
We apply this proof to argue \(D'C^{-1}D\) is full column rank, and hence invertible.
\end{note}

We can estimate the asymptotic variance of \(\sqrt{n}(\hat\beta_{2SLS}-\beta)\) by:
\[\hat V=\hat\sigma^2(\tfrac{1}{n}\hat X' \hat X)^{-1}\]
where \(\hat\sigma^2=\tfrac{1}{n}\hat\epsilon'\hat\epsilon\) and \(\hat\epsilon=Y-\hat X'\hat\beta_{2SLS}\)

Under (IV3') (hetereoskedasticity) we can use White's estimate as in earlier discussions:
\[\hat V_{het}= \frac{1}{n}\sum_{i=1}^n \hat\epsilon_i^2 w_i w_i'\]

Homoskedasticity or robust variance estimates of \(\hat\beta_{2SLS}\) can be used to form F-statistics for testing linear hypotheses in the usual way.
Asymptotically, such F-statistics would be distributed as \(\chi^2(p)/p\), where \(p\) is the number of restrictions. However, finite sample distribution of the F-statistics would not be \(F(p,n-k)\) even if \(\epsilon_i\) is normally distributed.

Asymptotically the Wald statistic for testing \(H_0:R\beta=r\) is:
\[W=(R\hat\beta_{2SLS}-r)'[R\hat V_{2SLS}R']^{-1}(R\hat\beta_{2SLS}-r)\xrightarrow{d}\chi^2(p)\]
where \(p\) is the number of restrictions.

\section{Control function approach}

This is an alternative approach to 2SLS, which is useful when we have multiple endogenous variables.

Consider again the model:
\[y_i=x_{1i}'\beta_1+x_{2i}'\beta_2+\epsilon_i\]
where \(x_{1i}\) is exogenous and \(x_{2i}\) is endogenous.
\\ Instead of extracting the exogenous part \(w_i'\pi\) of \(x_i\) and use it in the second stage, we could instead extract the endogenous part of \(x_i\) (the control function) and add it to the regression as an additional regressor.

\begin{theorem}
The two approaches are equivalent. \(\hat\beta_{CF}\equiv\hat\beta_{2SLS}\)
\end{theorem}
\vspace{5mm}
\begin{proof}
The exogenous part \(w_i'\pi\) of \(x_i\) is simply the best linear predictor of \(x_i\) given \(w_i\).

The first stage regression:
\[x_i'=w_i'\pi+u_i', \text{ where }\pi \text{ is } l\times k\]
is called a \textit{reduced form} regression, because it does not have any structural interpretation. We just want to predict \(x_i\) by a linear function of \(w_i\) in the best possible way (thus exogeneity is not required).
Recall \(w_i\) contains components of both included exogenous variables \(x_{1i}\) and excluded exogenous variables \(z_i\).

Thus we partition the reduced form equations into:
\[x_{1i}'=x_{1i}'\pi_{11}+z_i'\pi_{12}+u_{1i}'\]
\[x_{2i}'=x_{1i}'\pi_{21}+z_i'\pi_{22}+u_{2i}'\]
where \(\pi_{ij}\) is a \(k_j\times k_i\) matrix.

Of course the BLP of \(x_{1i}\) given \(x_{1i}\text{ and }z_{1i}\) is just \(x_{1i}\) so the first of the above equations is trivial \(x_{1i}'=x_{1i}'\)
For the second equation we drop the first subscript and rewrite as:
\[x_{2i}'=x_{1i}'\pi_{1}+z_i'\pi_{2}+u_{i}'\]

In 2SLS this regression would be estimated, obtain \(\hat x_{2i}'\), form \(\hat x_i\) by combining \(x_{1i},\text{ with }\hat x_{2i}\) and proceeding to second stage.

But note \(x_{2i}\) can only be endogenous if \(E(u_i\epsilon_i)\neq0\), that is, the error of the first stage \(u_i\) is correlated with the structural error \(\epsilon_i\).
Alternatively, note \(x_2i\) can only be endogenous if \(E(\vec u_i \epsilon_i)\neq\vec0\)*

That is, the error of the first stage regression, \(u_i\) is correlated with the structural error \(\epsilon_i\). 
The error \(u_i\) has \textit{soaked up} the endogeneity in \(x_{2i}\) thus adding it to the structural equation would control for the endogeneity and so get consistent estimates for the other structural parameters.

Consdier the BLP of \(\epsilon_i\) given \(u_i\):
\[\epsilon_i=u_i'\alpha+e_i\]
By definition the error of the BLP is uncorrelated to the dependent \(\epsilon_i\), else it would have been taken into account in the regression.

Substituting this into the strucutural equation, we obtain\[y_i=x_{1i}'\beta_1+x_{2i}'\beta_2+u_i'\alpha+e_i\]
where:
\[E(u_ie_i)=0\]
\[E(x_{1i}e_i)=E(x_{1i}(\epsilon_i-u_i'\alpha))=0\]
\[E(x_{2i}e_i)=E((\pi_1'x_{1i}+\pi_2'z_i+u_i)e_i)=E(\pi_2'z_ie_i)=\pi_2E(z_i(\epsilon-u_i'\alpha))=0\]

Thus OLS2' satisfied and the OLS estimates of \(\beta_1,\beta_2,\) and \(\alpha\) should be consistent. But we do not observe \(u_i\) so it must first be estimated from the first stage regression before insertion.

Let \(\hat U\) be the matrix with rows \(\hat u_i'\). Then by the partitioned regression formula (FW - theorem):
\[\hat\beta_{CF}\equiv(X'M_{\hat U}X)^{-1}X'M_{\hat U}X\]

But \(\hat U = M_WX_2\) so that:
\[M_{\hat U}=I-\hat U(\hat U'\hat U)^{-1}\hat U'= I-M_WX_2(X_2'M_WX_2)^{-1}X_2'M_W\]
Since \(X_1\) is a part of \(W\), \(M_WX_1=0\), and
\[M_{\hat U}X_1=X_1=P_WX_1\]
Further
\[M_{\hat U}X_2=X_2-M_WX_2(X_2'M_WX_2)^{-1}X_2'M_WX_2=P_WX_2\]
Therefore
\[M_{\hat U}X=P_WX\]
and so
\[\hat\beta_{CF}\equiv(X'M_{\hat U}X)^{-1}X'M_{\hat U}Y=(X'P_WX)^{-1}X'P_WY=\hat\beta_{2SLS}\]

*\(E(x_{2i} \epsilon_i)\neq\vec0 \implies E[(w_i'\pi + u_i')'\epsilon]\neq0 \implies \pi \cancel{E[w_i\epsilon]} + E[u_i\epsilon_i]\neq0\)

\end{proof}

\section{Endogeneity and Overidentification test}

\textit{Endogeneity test:} If \(x_{2i}\) is not endogenous, then OLS is efficient (BLUE) and 2SLS is not.

Test \[H_0: E(x_{2i}\epsilon_i)=0\text{ against }H_1: E(x_{2i}\epsilon_i)\neq0\]

Recall the CF regression:
\[y_i=x_{1i}'\beta_1+x_{2i}'\beta_2+u_i'\alpha+e_i\]
where \[ \alpha = E(u_iu_i')^{-1}E(u_i\epsilon_i) (\text{ the coefficient of BLP} for \epsilon_i given u_i)\]
We have \(E(x_{2i}\epsilon_i\neq0)\) if and only if \(E(u_i\epsilon_i)\neq0\). Therefore hypothesis test equivalent to:
\[H_0: \alpha=0 \text{ against }H_1: \alpha\neq0\]

Therefore a natural test would be the Wald statistic for testing linear restrictions \(\alpha=0\) in the control function regression, with \(u_i\) replaced with \(\hat u_i\).
It turns out this replacement des not affect the asymptotic distribution of the test stastistic under the null, and remains \(\chi^2(k_2)\) where \(k_2\) is the \(dim(\alpha)=dim(x_{2i})\) 
This follows from a general result on the asymptotic distribution of the OLS estimates of regression coefficients with 'generated' regressions (i.e. the hats consistently estimating the true) H(12-26,12-27)
In stata this occurs after estat endoggy WUFF WUFF after ivregress. Het robust s.e. then reported as 'robust regression F' otherwise if default daniel homoskedasticity then reported as 'Wu-Hausman F'

\textit{Overidentification test:}
With \(l>k\) (instruments > endoggy regressors) we can test the hypothesis that instruments are exogenous, that is
\[H_0: E(w_i\epsilon_i)=0\]
Let us assume the homoskedasticity, so that \(E(\epsilon_i^2|w_i)=\sigma^2\). Then consider a reduced form regression:
\[\epsilon_i=w_i'\alpha+e_i\],
where
\[\alpha=(E(w_iw_i'))^{-1}E(w_i\epsilon_i)\]
We see that \(E(w_i\epsilon_i)\neq0\) if and only if \(\alpha\neq0\).
We cannot regress \(\epsilon_i\) on \(w_i\) because we do not observe \(\epsilon_i\). But we can try to replace \(\epsilon_i\) with \(\hat \epsilon_i\),
(the residuals from the 2SLS esimtate of \(\beta\) NOTE this is not the same as the second stage residuals).

Sargan proposed to use \(nR^2\) from this regression as the test stat for \(H_0\) vs \(H_1\):
\[S=nR^2=n\frac{SSE}{SST}=n\frac{\hat\epsilon'W(W'W)^{-1}W'\hat\epsilon}{\hat\epsilon'\hat\epsilon}\]

Asymptotic Distribution of S:
Note S is invariant wrt transformations \(W \rightarrow W\times A\) where \(A\) is any invertible matrix. Therefore wlog we assume \(W\) rotated and scaled so that \(W(w_iw_i')=I_l\) As \(n\rightarrow\infty\):
\[\frac{1}{\sqrt{n}}W'\epsilon=\frac{1}{\sqrt{n}}\sum_{i=1}^n w_i\epsilon_i\xrightarrow{d}N(0,Var(w_i\epsilon_i))=N(0,\sigma^2I_l)=\sigma N(0,I_l)\]
\[\frac{1}{n}W'W\xrightarrow{p}E(w_iw_i')^{-1}=I_l\]
and \(\frac{1}{n}W'X\xrightarrow{p}E(w_ix_i')=Q\) where \(Q\) is some full column rank matrix. On the other hand:
\[\frac{1}{\sqrt{n}}W'\hat\epsilon=\frac{1}{\sqrt{n}}W'(Y-X\hat\beta_{2SLS})=\frac{1}{\sqrt{n}}W'(Y-X(X'P_WX)^{-1}X'P_WY)\]
\[=\frac{1}{\sqrt{n}}W'(\epsilon + X(X'P_WX)^{-1}X'P_W\epsilon)\]
\[=(I-W'X(X'P_WX)^{-1}X'P_W)\frac{1}{\sqrt{n}}W'\epsilon\]
\[\xrightarrow{d} (I-Q(Q'Q)^{-1}Q')\sigma N(0,I_l)\]
Therefore,
\[\hat\epsilon'W(W'W)^{-1}W'\hat\epsilon=\frac{1}{\sqrt{n}}\hat\epsilon'W(\frac{1}{n}W'W)^{-1}\frac{1}{\sqrt{n}}W'\hat\epsilon\]
\[\xrightarrow{d} \sigma^2N'(I-Q(Q'Q)^{-1}Q') N\]

\begin{lemma}
    \(N'(I-Q(Q'Q)^{-1}Q') N\sim\chi^2(l-k)\)
\end{lemma}
\vspace{5mm}
\begin{proof}
We have \(Q'Q=I_k\) and \(Q: l\times k\) where \(l>k\)
We define \(Q_c\) as the \(l\times (l-k)\) orthonormal complement matrix such that \([Q \, Q_c]\) together form an \(l\times l\) complete orthogonal matrix.
Thus \([Q \, Q_c][Q \, Q_c]'=I_l\)
\[\implies QQ' + Q_cQ_c'=I_l\]
\[\implies Q_cQ_c'=I_l-QQ'\]
Thus \[N'(I-Q(Q'Q)^{-1}Q') N= N'Q_cQ_c'N\]
\[=(Q_c'N)'(Q_c'N)\]
But \(Q_c'N \sim N(0,Q_c'I_lQ_c)=N(0,I_{l-k})\)
Thus \[(Q_c'N)'(Q_c'N)=\sum_{i=1}^{l-k}(z_i)^2\sim\chi^2(l-k)\]
\end{proof}

Thus:
\[\hat\epsilon'W(W'W)^{-1}W'\hat\epsilon\xrightarrow{d}\sigma^2\chi^2(l-k)\]
Finally, \(\frac{\hat\epsilon'\hat\epsilon}{n}\xrightarrow{p}\sigma^2\) (sim to lec 8 proof)
Therefore:
\[S=n\frac{\hat\epsilon'W(W'W)^{-1}W'\hat\epsilon}{\hat\epsilon'\hat\epsilon}\xrightarrow{d}\chi^2(l-k)\]
We reject the null of the instrument exogeneity when \(s\) is larger than a critical value of \(\chi^2(l-k)\)
\begin{note}
The test cannot be performed in the just-identified situation (\(l=k)\).
Then \(W'X\) has full rank and so is thus invertible.

\[\frac{1}{\sqrt{n}}W'\hat\epsilon=(I-W'X(X'P_WX)^{-1}X'W(W'W)^{-1})\frac{1}{\sqrt{n}}W'\epsilon\]
\[=(I-W'X(X'W(W'W)^{-1}W'X)^{-1}X'W(W'W)^{-1})\frac{1}{\sqrt{n}}W'\epsilon\]
\[=(I-W'X(W'X)^{-1}W'W(X'W)^{-1}X'W(W'W)^{-1})\frac{1}{\sqrt{n}}W'\epsilon\]
\[(I-I)\frac{1}{\sqrt{n}}W'\epsilon=0\]

\end{note}
\section{Appendix}
\subsection{Chi-squared asymptotic result}
\vspace{5mm}
\begin{lemma}
For \(\vec z\sim N(0,V)\)
We have \[z'V^{-1}z\xrightarrow{d}\chi^2(p)\]
where \(p\) is the number of elements in \(z\).
\end{lemma}
\vspace{5mm}
\begin{proof}
    As \(V\) symmetric we can write its spectral decomposition:
    \[V=Q\Lambda Q'=Q\Lambda^{1/2}\Lambda^{1/2}Q'\]
    where \(Q\) orthogonal and \(\Lambda\) diagonal with eigenvalues \(\lambda_1,...,\lambda_p\).
    \[\therefore z'V^{-1}z=z'(Q\Lambda^{1/2}\Lambda^{1/2}Q')^{-1}z\]
    \[=((\Lambda^{1/2}Q)^{-1}z)'((\Lambda^{1/2}Q)^{-1}z)\]
    But
    \[(\Lambda^{1/2}Q)^{-1}z\sim N(0,(\Lambda^{1/2}Q)^{-1}V(Q'\Lambda^{1/2})^{-1})\]
    \[=N(0,(\Lambda^{1/2}Q)^{-1}Q\Lambda^{1/2}\Lambda^{1/2}Q'(Q'\Lambda^{1/2})^{-1})\]
    \[=N(0,I_p)\]
    Therefore \((\Lambda^{1/2}Q)^{-1}z\) is a vector of \(p\) independent standard normals.
    \\ Therefore \(((\Lambda^{1/2}Q)^{-1}z)'((\Lambda^{1/2}Q)^{-1}z)\) is the sum of \(p\) independent standard normals squared, which is \(\chi^2(p)\). 
\end{proof}

\subsection{Limited Info Maximum Likelihood}
- no finite sample moments the same as 2SLS (so will have outliers)
\\ - but better than 2sls with weak instruments

Recall the same linear regression model:

\[y_i=x_i'\beta+\epsilon_i\]
\[x_i'=w_i'\pi+u_i'\]
\[\implies y_i=w_i'\pi\beta+u_i'\beta+\epsilon_i\]
Let \((y_i,x_i) = Y_i'\)
\[\implies Y_i'=w_i'(\pi\beta, \pi)+(u_i'\beta+\epsilon_i, u_i')\]
Transposing
\[Y_i=\underbrace{\begin{pmatrix} \beta'\pi' \\ \pi'\end{pmatrix}}_{\Gamma(\beta,\pi)}w_i+\underbrace{\begin{pmatrix} \beta'u_i+\epsilon_i \\ u_i\end{pmatrix}}_{e_i}\]
\[\implies Y_i=\Gamma(\beta,\pi)w_i+e_i\]
Assume:
\[e_i|w_i\sim N(0,\Omega)\]

We can then write likelihood function, and maximise wrt parameters to find
\(\hat \beta_{ML}=\hat \beta_{LIML}, \hat \pi_{ML}\) and \(\hat \Omega_{ML}\)
\end{document}